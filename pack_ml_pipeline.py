#!/usr/bin/env python3
"""
Pack SGA ML Pipeline - Predict %Pack
Complete end-to-end machine learning pipeline
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import shap
import pickle
import warnings
warnings.filterwarnings('ignore')

# Set Minitab-style plotting
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("="*80)
print("PACK SGA ML PIPELINE - %PACK PREDICTION")
print("="*80)

# ============================================================================
# STEP 1: LOAD & CLEAN DATA
# ============================================================================
print("\n[STEP 1/6] Loading and Cleaning Data...")
df = pd.read_excel('/mnt/project/Pack_SGA_2425.xlsx')
print(f"‚úì Loaded {df.shape[0]} records, {df.shape[1]} columns")

# Check missing values
missing = df.isnull().sum()
if missing.sum() == 0:
    print("‚úì No missing values detected")
else:
    print(f"‚ö† Missing values found:\n{missing[missing > 0]}")
    df = df.dropna()
    print(f"‚úì Cleaned to {df.shape[0]} records")

# Check outliers using IQR method for %Pack
Q1 = df['%Pack'].quantile(0.25)
Q3 = df['%Pack'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers = df[(df['%Pack'] < lower_bound) | (df['%Pack'] > upper_bound)]
print(f"‚úì Outliers detected: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)")

# Keep all data but note outliers
print(f"‚úì Target range: {df['%Pack'].min():.4f} - {df['%Pack'].max():.4f}")

# ============================================================================
# STEP 2: FEATURE ENGINEERING & PREPROCESSING
# ============================================================================
print("\n[STEP 2/6] Feature Engineering & Preprocessing...")

# Extract date features
df['Year'] = df['FullDate'].dt.year
df['Month'] = df['FullDate'].dt.month
df['Day'] = df['FullDate'].dt.day
df['DayOfWeek'] = df['FullDate'].dt.dayofweek
df['Quarter'] = df['FullDate'].dt.quarter

# Separate features and target
target_col = '%Pack'
exclude_cols = ['FullDate', '%Pack', 'Pack (Ton)']
feature_cols = [col for col in df.columns if col not in exclude_cols]

X = df[feature_cols].copy()
y = df[target_col].copy()

print(f"‚úì Features: {len(feature_cols)}")
print(f"  Numerical: {X.select_dtypes(include=[np.number]).columns.tolist()}")
print(f"  Categorical: {X.select_dtypes(include=['object']).columns.tolist()}")

# Encode categorical variables
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    label_encoders[col] = le
    print(f"‚úì Encoded {col}: {len(le.classes_)} unique values")

# Store feature names after encoding
feature_names = X.columns.tolist()

# Normalize features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=feature_names)
print(f"‚úì Features normalized using MinMaxScaler")

# Train-test split (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)
print(f"‚úì Train set: {X_train.shape[0]} samples")
print(f"‚úì Test set: {X_test.shape[0]} samples")

# ============================================================================
# STEP 3: MODEL TRAINING & COMPARISON
# ============================================================================
print("\n[STEP 3/6] Training & Comparing Models...")

models = {
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=20, 
                                          min_samples_split=5, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, 
                                                    learning_rate=0.1, random_state=42),
    'Ridge Regression': Ridge(alpha=1.0, random_state=42)
}

results = {}
predictions = {}

for name, model in models.items():
    print(f"\n--- {name} ---")
    
    # Train
    model.fit(X_train, y_train)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, 
                               scoring='r2', n_jobs=-1)
    
    # Predictions
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    # Metrics
    r2_train = r2_score(y_train, y_pred_train)
    r2_test = r2_score(y_test, y_pred_test)
    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
    mae_test = mean_absolute_error(y_test, y_pred_test)
    
    results[name] = {
        'model': model,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'r2_train': r2_train,
        'r2_test': r2_test,
        'rmse': rmse_test,
        'mae': mae_test
    }
    
    predictions[name] = {
        'train': y_pred_train,
        'test': y_pred_test
    }
    
    print(f"  CV R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
    print(f"  Train R¬≤: {r2_train:.4f}")
    print(f"  Test R¬≤: {r2_test:.4f}")
    print(f"  RMSE: {rmse_test:.4f}")
    print(f"  MAE: {mae_test:.4f}")

# Select best model
best_model_name = max(results, key=lambda x: results[x]['r2_test'])
best_model = results[best_model_name]['model']
print(f"\n‚úì Best Model: {best_model_name} (Test R¬≤ = {results[best_model_name]['r2_test']:.4f})")

# ============================================================================
# STEP 4: VISUALIZATIONS (Minitab Style)
# ============================================================================
print("\n[STEP 4/6] Creating Visualizations (Minitab Style)...")

# Set consistent figure style
def set_minitab_style(ax, title, xlabel, ylabel):
    ax.set_title(title, fontsize=12, fontweight='bold', pad=15)
    ax.set_xlabel(xlabel, fontsize=10, fontweight='bold')
    ax.set_ylabel(ylabel, fontsize=10, fontweight='bold')
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

# (1) Scatter: Actual vs Predicted
print("  Creating (1/9) Scatter Plot...")
fig, ax = plt.subplots(figsize=(8, 6))
y_pred_best = predictions[best_model_name]['test']
ax.scatter(y_test, y_pred_best, alpha=0.5, s=30, edgecolors='black', linewidth=0.5)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
        'r--', lw=2, label='Perfect Prediction')
set_minitab_style(ax, f'{best_model_name}: Actual vs Predicted %Pack',
                 'Actual %Pack', 'Predicted %Pack')
ax.legend(loc='upper left', frameon=True)
r2 = results[best_model_name]['r2_test']
ax.text(0.05, 0.95, f'R¬≤ = {r2:.4f}', transform=ax.transAxes, 
        fontsize=11, verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
plt.tight_layout()
plt.savefig('/home/claude/01_scatter_actual_vs_predicted.png', dpi=300, bbox_inches='tight')
plt.close()

# (2) Feature Importance
print("  Creating (2/9) Feature Importance...")
if hasattr(best_model, 'feature_importances_'):
    importances = best_model.feature_importances_
    indices = np.argsort(importances)[::-1][:15]  # Top 15
    
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.barh(range(len(indices)), importances[indices], color='steelblue', edgecolor='black')
    ax.set_yticks(range(len(indices)))
    ax.set_yticklabels([feature_names[i] for i in indices])
    set_minitab_style(ax, 'Feature Importance (Top 15)', 
                     'Importance Score', 'Features')
    ax.invert_yaxis()
    plt.tight_layout()
    plt.savefig('/home/claude/02_feature_importance.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Store for Pareto
    feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values('Importance', ascending=False)
else:
    print("  ‚ö† Model doesn't have feature_importances_")
    feature_importance_df = None

# (3) SHAP Summary
print("  Creating (3/9) SHAP Summary...")
try:
    explainer = shap.TreeExplainer(best_model)
    shap_values = explainer.shap_values(X_test[:1000])  # Sample for speed
    
    fig, ax = plt.subplots(figsize=(10, 8))
    shap.summary_plot(shap_values, X_test[:1000], feature_names=feature_names, 
                     show=False, plot_type='dot')
    plt.title('SHAP Summary Plot - Feature Impact on %Pack', 
              fontsize=12, fontweight='bold', pad=15)
    plt.tight_layout()
    plt.savefig('/home/claude/03_shap_summary.png', dpi=300, bbox_inches='tight')
    plt.close()
except Exception as e:
    print(f"  ‚ö† SHAP failed: {e}")

# (4) Pareto Chart
print("  Creating (4/9) Pareto Chart...")
if feature_importance_df is not None:
    top_features = feature_importance_df.head(15).copy()
    top_features['Cumulative %'] = top_features['Importance'].cumsum() / top_features['Importance'].sum() * 100
    
    fig, ax1 = plt.subplots(figsize=(12, 6))
    ax2 = ax1.twinx()
    
    x_pos = np.arange(len(top_features))
    ax1.bar(x_pos, top_features['Importance'], color='steelblue', 
            edgecolor='black', alpha=0.7, label='Importance')
    ax2.plot(x_pos, top_features['Cumulative %'], color='red', 
             marker='o', linewidth=2, markersize=6, label='Cumulative %')
    ax2.axhline(y=80, color='green', linestyle='--', linewidth=1.5, 
                label='80% Line')
    
    ax1.set_xlabel('Features', fontsize=10, fontweight='bold')
    ax1.set_ylabel('Importance Score', fontsize=10, fontweight='bold', color='steelblue')
    ax2.set_ylabel('Cumulative Percentage (%)', fontsize=10, fontweight='bold', color='red')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(top_features['Feature'], rotation=45, ha='right')
    ax1.set_title('Pareto Chart - Feature Importance', fontsize=12, fontweight='bold', pad=15)
    ax1.grid(True, alpha=0.3, linestyle='--')
    ax1.legend(loc='upper left')
    ax2.legend(loc='upper right')
    
    plt.tight_layout()
    plt.savefig('/home/claude/04_pareto_chart.png', dpi=300, bbox_inches='tight')
    plt.close()

# (5) Correlation Matrix
print("  Creating (5/9) Correlation Matrix...")
corr_data = X_scaled.copy()
corr_data['%Pack'] = y.values
corr_matrix = corr_data.corr()

fig, ax = plt.subplots(figsize=(14, 12))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0,
            square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
ax.set_title('Correlation Matrix - All Features', fontsize=12, fontweight='bold', pad=15)
plt.tight_layout()
plt.savefig('/home/claude/05_correlation_matrix.png', dpi=300, bbox_inches='tight')
plt.close()

# (6) Residual Plot
print("  Creating (6/9) Residual Plot...")
residuals = y_test - y_pred_best
fig, ax = plt.subplots(figsize=(8, 6))
ax.scatter(y_pred_best, residuals, alpha=0.5, s=30, edgecolors='black', linewidth=0.5)
ax.axhline(y=0, color='red', linestyle='--', linewidth=2)
set_minitab_style(ax, 'Residual Plot', 'Predicted %Pack', 'Residuals')
plt.tight_layout()
plt.savefig('/home/claude/06_residual_plot.png', dpi=300, bbox_inches='tight')
plt.close()

# (7) Residual Histogram
print("  Creating (7/9) Residual Histogram...")
fig, ax = plt.subplots(figsize=(8, 6))
ax.hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
ax.axvline(x=0, color='red', linestyle='--', linewidth=2)
set_minitab_style(ax, 'Residual Distribution', 'Residuals', 'Frequency')
ax.text(0.05, 0.95, f'Mean: {residuals.mean():.6f}\nStd: {residuals.std():.6f}',
        transform=ax.transAxes, fontsize=10, verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
plt.tight_layout()
plt.savefig('/home/claude/07_residual_histogram.png', dpi=300, bbox_inches='tight')
plt.close()

# (8) Main Effect Plot (All Features vs %Pack)
print("  Creating (8/9) Main Effect Plot...")
# Select top features for clarity
top_n_features = 6
if feature_importance_df is not None:
    top_feature_names = feature_importance_df.head(top_n_features)['Feature'].tolist()
else:
    top_feature_names = feature_names[:top_n_features]

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for idx, feature in enumerate(top_feature_names):
    ax = axes[idx]
    
    # Get original feature values (before scaling)
    feature_values = X[feature].values
    target_values = y.values
    
    # Create bins and calculate means
    n_bins = 10
    try:
        bins = pd.qcut(feature_values, q=n_bins, duplicates='drop')
        bin_means_x = []
        bin_means_y = []
        
        for bin_label in bins.cat.categories:
            mask = bins == bin_label
            if mask.sum() > 0:
                bin_means_x.append(feature_values[mask].mean())
                bin_means_y.append(target_values[mask].mean())
    except Exception:
        # Fallback: use simple binning
        bins = pd.cut(feature_values, bins=n_bins)
        grouped = pd.DataFrame({'x': feature_values, 'y': target_values, 'bin': bins})
        agg = grouped.groupby('bin').agg({'x': 'mean', 'y': 'mean'}).dropna()
        bin_means_x = agg['x'].tolist()
        bin_means_y = agg['y'].tolist()
    
    # Plot line (Minitab style)
    ax.plot(bin_means_x, bin_means_y, marker='o', linewidth=2, 
            markersize=6, color='steelblue')
    ax.set_xlabel(feature, fontsize=9, fontweight='bold')
    ax.set_ylabel('Mean %Pack', fontsize=9, fontweight='bold')
    ax.set_title(f'Main Effect: {feature}', fontsize=10, fontweight='bold')
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

plt.suptitle('Main Effect Plots - Feature Impact on %Pack', 
             fontsize=14, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig('/home/claude/08_main_effect_plot.png', dpi=300, bbox_inches='tight')
plt.close()

# (9) Main Interaction Plot (Top 2 features)
print("  Creating (9/9) Main Interaction Plot...")
if feature_importance_df is not None and len(top_feature_names) >= 2:
    feat1, feat2 = top_feature_names[0], top_feature_names[1]
    
    # Create interaction bins
    try:
        feat1_bins = pd.qcut(X[feat1], q=4, duplicates='drop')
        feat2_bins = pd.qcut(X[feat2], q=4, duplicates='drop')
        
        # Map to labels
        feat1_labels = pd.qcut(X[feat1], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'], duplicates='drop')
        feat2_labels = pd.qcut(X[feat2], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'], duplicates='drop')
    except Exception:
        # Fallback to cut
        feat1_labels = pd.cut(X[feat1], bins=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
        feat2_labels = pd.cut(X[feat2], bins=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
    
    interaction_df = pd.DataFrame({
        'feat1': feat1_labels,
        'feat2': feat2_labels,
        'target': y.values
    })
    
    # Calculate means
    interaction_means = interaction_df.groupby(['feat1', 'feat2'])['target'].mean().unstack()
    
    fig, ax = plt.subplots(figsize=(10, 6))
    for col in interaction_means.columns:
        ax.plot(interaction_means.index, interaction_means[col], 
                marker='o', linewidth=2, markersize=6, label=f'{feat2}={col}')
    
    ax.set_xlabel(feat1, fontsize=10, fontweight='bold')
    ax.set_ylabel('Mean %Pack', fontsize=10, fontweight='bold')
    ax.set_title(f'Interaction Plot: {feat1} √ó {feat2}', 
                fontsize=12, fontweight='bold', pad=15)
    ax.legend(title=feat2, frameon=True, loc='best')
    ax.grid(True, alpha=0.3, linestyle='--')
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    
    plt.tight_layout()
    plt.savefig('/home/claude/09_interaction_plot.png', dpi=300, bbox_inches='tight')
    plt.close()

print("‚úì All 9 visualizations created successfully!")

# ============================================================================
# STEP 5: EXPORT RESULTS
# ============================================================================
print("\n[STEP 5/6] Exporting Results...")

# Save best model and preprocessing objects
model_package = {
    'model': best_model,
    'scaler': scaler,
    'label_encoders': label_encoders,
    'feature_names': feature_names,
    'categorical_cols': categorical_cols,
    'best_model_name': best_model_name,
    'metrics': results[best_model_name]
}

with open('/home/claude/pack_model.pkl', 'wb') as f:
    pickle.dump(model_package, f)
print("‚úì Model saved: pack_model.pkl")

# Export summary to Excel
summary_data = []
for name, res in results.items():
    summary_data.append({
        'Model': name,
        'CV_R2_Mean': res['cv_mean'],
        'CV_R2_Std': res['cv_std'],
        'Train_R2': res['r2_train'],
        'Test_R2': res['r2_test'],
        'RMSE': res['rmse'],
        'MAE': res['mae']
    })

summary_df = pd.DataFrame(summary_data)
with pd.ExcelWriter('/home/claude/model_summary.xlsx', engine='openpyxl') as writer:
    summary_df.to_excel(writer, sheet_name='Model_Comparison', index=False)
    
    if feature_importance_df is not None:
        feature_importance_df.to_excel(writer, sheet_name='Feature_Importance', index=False)
        
        # Add Pareto data
        pareto_df = feature_importance_df.head(15).copy()
        pareto_df['Cumulative_Importance'] = pareto_df['Importance'].cumsum()
        pareto_df['Cumulative_Percentage'] = pareto_df['Cumulative_Importance'] / pareto_df['Importance'].sum() * 100
        pareto_df.to_excel(writer, sheet_name='Pareto_Data', index=False)

print("‚úì Summary saved: model_summary.xlsx")

# ============================================================================
# STEP 6: CREATE STREAMLIT APP
# ============================================================================
print("\n[STEP 6/6] Creating Streamlit Deployment App...")

streamlit_code = '''import streamlit as st
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns

st.set_page_config(page_title="Pack SGA %Pack Predictor", layout="wide", page_icon="üì¶")

# Load model
@st.cache_resource
def load_model():
    with open('pack_model.pkl', 'rb') as f:
        return pickle.load(f)

model_package = load_model()
model = model_package['model']
scaler = model_package['scaler']
label_encoders = model_package['label_encoders']
feature_names = model_package['feature_names']
categorical_cols = model_package['categorical_cols']
best_model_name = model_package['best_model_name']
metrics = model_package['metrics']

# Sidebar
st.sidebar.title("üì¶ Pack SGA Predictor")
st.sidebar.markdown("---")
page = st.sidebar.radio("Navigation", 
                       ["üîÆ Single Prediction", 
                        "üìä Batch Prediction", 
                        "üìà Model Analytics",
                        "‚ÑπÔ∏è About"])

# ============================================================================
# PAGE 1: Single Prediction
# ============================================================================
if page == "üîÆ Single Prediction":
    st.title("üîÆ Single %Pack Prediction")
    st.markdown("Enter production parameters to predict packing efficiency")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.subheader("Date & Shift")
        year = st.number_input("Year", min_value=2024, max_value=2030, value=2025)
        month = st.slider("Month", 1, 12, 6)
        day = st.slider("Day", 1, 31, 15)
        day_of_week = st.selectbox("Day of Week", list(range(7)), 
                                   format_func=lambda x: ['Mon','Tue','Wed','Thu','Fri','Sat','Sun'][x])
        quarter = st.selectbox("Quarter", [1, 2, 3, 4])
        shift = st.selectbox("Shift", list(label_encoders['Shift'].classes_))
    
    with col2:
        st.subheader("Equipment")
        furnace = st.number_input("Furnace", min_value=1, max_value=5, value=2)
        line = st.number_input("Line", min_value=1, max_value=10, value=1)
    
    with col3:
        st.subheader("Product Info")
        customer = st.selectbox("Customer", list(label_encoders['Customer'].classes_))
        product_type = st.selectbox("Type", list(label_encoders['Type'].classes_))
        description = st.selectbox("Description", list(label_encoders['Description'].classes_))
        total_fg = st.number_input("Total FG (Bottle)", min_value=0, max_value=500000, value=200000)
    
    if st.button("üéØ Predict %Pack", type="primary", use_container_width=True):
        try:
            # Prepare input
            input_data = pd.DataFrame({
                'Shift': [shift],
                'Furnace': [furnace],
                'Line': [line],
                'Customer': [customer],
                'Type': [product_type],
                'Description': [description],
                'Total FG (Bottle)': [total_fg],
                'Year': [year],
                'Month': [month],
                'Day': [day],
                'DayOfWeek': [day_of_week],
                'Quarter': [quarter]
            })
            
            # Encode categorical
            for col in categorical_cols:
                input_data[col] = label_encoders[col].transform(input_data[col].astype(str))
            
            # Ensure correct order
            input_data = input_data[feature_names]
            
            # Scale
            input_scaled = scaler.transform(input_data)
            
            # Predict
            prediction = model.predict(input_scaled)[0]
            
            # Display result
            st.markdown("---")
            st.markdown("### üéØ Prediction Result")
            
            col1, col2, col3 = st.columns(3)
            col1.metric("Predicted %Pack", f"{prediction:.4f}", f"{prediction*100:.2f}%")
            col2.metric("Model Used", best_model_name)
            col3.metric("Model R¬≤ Score", f"{metrics['r2_test']:.4f}")
            
            # Interpretation
            if prediction >= 0.92:
                st.success("‚úÖ Excellent packing efficiency! Above target.")
            elif prediction >= 0.88:
                st.info("‚ÑπÔ∏è Good packing efficiency. Within acceptable range.")
            else:
                st.warning("‚ö†Ô∏è Below average efficiency. Consider investigating factors.")
                
        except Exception as e:
            st.error(f"‚ùå Error: {str(e)}")

# ============================================================================
# PAGE 2: Batch Prediction
# ============================================================================
elif page == "üìä Batch Prediction":
    st.title("üìä Batch %Pack Prediction")
    st.markdown("Upload CSV/Excel file for batch predictions")
    
    uploaded_file = st.file_uploader("Upload File (CSV or Excel)", type=['csv', 'xlsx'])
    
    if uploaded_file is not None:
        try:
            if uploaded_file.name.endswith('.csv'):
                df_batch = pd.read_csv(uploaded_file)
            else:
                df_batch = pd.read_excel(uploaded_file)
            
            st.success(f"‚úÖ Loaded {len(df_batch)} records")
            st.dataframe(df_batch.head(10))
            
            if st.button("üöÄ Run Batch Prediction", type="primary"):
                with st.spinner("Processing..."):
                    # Process date features if FullDate exists
                    if 'FullDate' in df_batch.columns:
                        df_batch['FullDate'] = pd.to_datetime(df_batch['FullDate'])
                        df_batch['Year'] = df_batch['FullDate'].dt.year
                        df_batch['Month'] = df_batch['FullDate'].dt.month
                        df_batch['Day'] = df_batch['FullDate'].dt.day
                        df_batch['DayOfWeek'] = df_batch['FullDate'].dt.dayofweek
                        df_batch['Quarter'] = df_batch['FullDate'].dt.quarter
                    
                    # Select and prepare features
                    X_batch = df_batch[feature_names].copy()
                    
                    # Encode categorical
                    for col in categorical_cols:
                        X_batch[col] = label_encoders[col].transform(X_batch[col].astype(str))
                    
                    # Scale
                    X_batch_scaled = scaler.transform(X_batch)
                    
                    # Predict
                    predictions = model.predict(X_batch_scaled)
                    df_batch['Predicted_%Pack'] = predictions
                    df_batch['Predicted_%Pack_Pct'] = predictions * 100
                    
                    st.success("‚úÖ Predictions completed!")
                    
                    # Show results
                    st.dataframe(df_batch[['Predicted_%Pack', 'Predicted_%Pack_Pct']].head(20))
                    
                    # Summary statistics
                    col1, col2, col3, col4 = st.columns(4)
                    col1.metric("Mean", f"{predictions.mean():.4f}")
                    col2.metric("Median", f"{np.median(predictions):.4f}")
                    col3.metric("Min", f"{predictions.min():.4f}")
                    col4.metric("Max", f"{predictions.max():.4f}")
                    
                    # Download button
                    csv = df_batch.to_csv(index=False)
                    st.download_button(
                        label="üì• Download Results (CSV)",
                        data=csv,
                        file_name="pack_predictions.csv",
                        mime="text/csv"
                    )
                    
        except Exception as e:
            st.error(f"‚ùå Error: {str(e)}")

# ============================================================================
# PAGE 3: Model Analytics
# ============================================================================
elif page == "üìà Model Analytics":
    st.title("üìà Model Performance & Analytics")
    
    tab1, tab2 = st.tabs(["üìä Model Metrics", "üñºÔ∏è Visualizations"])
    
    with tab1:
        st.subheader("Model Comparison")
        
        # Load summary
        try:
            summary_df = pd.read_excel('model_summary.xlsx', sheet_name='Model_Comparison')
            st.dataframe(summary_df.style.highlight_max(axis=0, subset=['Test_R2'], color='lightgreen'))
            
            # Best model highlight
            st.success(f"üèÜ Best Model: **{best_model_name}** with Test R¬≤ = **{metrics['r2_test']:.4f}**")
            
            col1, col2, col3 = st.columns(3)
            col1.metric("Cross-Val R¬≤", f"{metrics['cv_mean']:.4f} ¬± {metrics['cv_std']:.4f}")
            col2.metric("RMSE", f"{metrics['rmse']:.4f}")
            col3.metric("MAE", f"{metrics['mae']:.4f}")
            
        except Exception as e:
            st.warning(f"Summary file not found: {e}")
    
    with tab2:
        st.subheader("Model Visualizations")
        
        # Display charts
        viz_files = [
            "01_scatter_actual_vs_predicted.png",
            "02_feature_importance.png",
            "03_shap_summary.png",
            "04_pareto_chart.png",
            "05_correlation_matrix.png",
            "06_residual_plot.png",
            "07_residual_histogram.png",
            "08_main_effect_plot.png",
            "09_interaction_plot.png"
        ]
        
        viz_titles = [
            "Actual vs Predicted",
            "Feature Importance",
            "SHAP Summary",
            "Pareto Chart",
            "Correlation Matrix",
            "Residual Plot",
            "Residual Histogram",
            "Main Effect Plot",
            "Interaction Plot"
        ]
        
        for viz_file, viz_title in zip(viz_files, viz_titles):
            try:
                st.image(viz_file, caption=viz_title, use_container_width=True)
            except:
                st.warning(f"Chart not found: {viz_file}")

# ============================================================================
# PAGE 4: About
# ============================================================================
else:
    st.title("‚ÑπÔ∏è About This Application")
    
    st.markdown("""
    ### üì¶ Pack SGA %Pack Predictor
    
    **Purpose:** Predict packing efficiency (%Pack) based on production parameters
    
    **Model Details:**
    - **Best Model:** {0}
    - **Test R¬≤ Score:** {1:.4f}
    - **Features:** {2}
    - **Training Samples:** 7,059
    - **Test Samples:** 1,765
    
    **Key Features:**
    - Single prediction for real-time estimation
    - Batch prediction for large-scale analysis
    - Interactive visualizations (9 charts)
    - Model performance metrics
    
    **Created by:** Kyoko (MIT USA)
    **Framework:** Streamlit + Scikit-learn
    **Version:** 1.0
    
    ---
    üí° **Usage Tips:**
    1. Use single prediction for quick estimates
    2. Upload batch files with same column structure as training data
    3. Check analytics for model insights
    4. Download predictions for further analysis
    
    üîí **Data Security:** All predictions run locally - no data sent to external servers
    """.format(best_model_name, metrics['r2_test'], len(feature_names)))
    
    st.markdown("---")
    st.info("üìß For support or questions, contact your data science team")

# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("**Model Info:**")
st.sidebar.markdown(f"‚úì {best_model_name}")
st.sidebar.markdown(f"‚úì R¬≤ = {metrics['r2_test']:.4f}")
st.sidebar.markdown(f"‚úì Features: {len(feature_names)}")
'''

with open('/home/claude/app.py', 'w') as f:
    f.write(streamlit_code)

print("‚úì Streamlit app created: app.py")

# Create requirements.txt
requirements = """streamlit==1.28.0
pandas==2.1.1
numpy==1.24.3
scikit-learn==1.3.0
matplotlib==3.7.2
seaborn==0.12.2
openpyxl==3.1.2
"""

with open('/home/claude/requirements.txt', 'w') as f:
    f.write(requirements)

print("‚úì Requirements file created: requirements.txt")

# Create README
readme = """# Pack SGA %Pack Prediction - ML Deployment

## Overview
Machine learning model to predict packing efficiency (%Pack) in SGA production.

## Model Performance
- **Best Model:** {0}
- **Test R¬≤ Score:** {1:.4f}
- **RMSE:** {2:.4f}
- **MAE:** {3:.4f}

## Files Included
1. `pack_model.pkl` - Trained model with preprocessing pipeline
2. `app.py` - Streamlit web application
3. `requirements.txt` - Python dependencies
4. `model_summary.xlsx` - Model comparison & feature importance
5. `01-09_*.png` - 9 visualization charts (Minitab style)

## Quick Start

### Local Deployment
```bash
pip install -r requirements.txt
streamlit run app.py
```

### Cloud Deployment (Recommended)

**Option 1: Streamlit Cloud (Free)**
1. Push to GitHub
2. Visit share.streamlit.io
3. Connect repository
4. Deploy!

**Option 2: Hugging Face Spaces**
1. Create account on huggingface.co
2. New Space ‚Üí Streamlit
3. Upload files
4. Auto-deploy

**Option 3: Railway**
1. Sign up at railway.app
2. New Project ‚Üí Deploy from GitHub
3. Set start command: `streamlit run app.py --server.port $PORT`

## Features
- üîÆ Single prediction with interactive inputs
- üìä Batch prediction from CSV/Excel
- üìà Model analytics & visualizations
- üì• Download prediction results

## Usage
1. Navigate to "Single Prediction" for individual estimates
2. Use "Batch Prediction" for multiple records
3. Check "Model Analytics" for performance insights

## Support
Contact your data science team for assistance.

**Created by:** Kyoko (MIT USA)
**Date:** {4}
""".format(best_model_name, 
          results[best_model_name]['r2_test'], 
          results[best_model_name]['rmse'], 
          results[best_model_name]['mae'],
          pd.Timestamp.now().strftime('%Y-%m-%d'))

with open('/home/claude/README.md', 'w') as f:
    f.write(readme)

print("‚úì README created: README.md")

print("\n" + "="*80)
print("‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")
print("="*80)
print(f"\nüìä Model Performance Summary:")
print(f"  Best Model: {best_model_name}")
print(f"  Test R¬≤: {results[best_model_name]['r2_test']:.4f}")
print(f"  RMSE: {results[best_model_name]['rmse']:.4f}")
print(f"  MAE: {results[best_model_name]['mae']:.4f}")

print(f"\nüì¶ Output Files:")
print(f"  ‚úì pack_model.pkl - Trained model package")
print(f"  ‚úì model_summary.xlsx - Model comparison & data")
print(f"  ‚úì app.py - Streamlit deployment app")
print(f"  ‚úì requirements.txt - Python dependencies")
print(f"  ‚úì README.md - Deployment guide")
print(f"  ‚úì 01-09_*.png - 9 visualization charts")

print(f"\nüöÄ Ready for deployment!")
print(f"   Run: streamlit run app.py")
print("="*80)
